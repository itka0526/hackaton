{"cells":[{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2022-07-25T09:03:58.686224Z","iopub.status.busy":"2022-07-25T09:03:58.68583Z","iopub.status.idle":"2022-07-25T09:04:00.780557Z","shell.execute_reply":"2022-07-25T09:04:00.779477Z","shell.execute_reply.started":"2022-07-25T09:03:58.686182Z"},"trusted":true},"outputs":[],"source":["import os\n","import random\n","import string\n","import math\n","from collections import Counter\n","from time import time\n","\n","import Augmentor\n","import numpy as np\n","import pandas as pd\n","import torch\n","from torch import nn\n","from torch.nn import Conv2d, MaxPool2d, BatchNorm2d, LeakyReLU\n","from torchvision import transforms\n","import matplotlib.pyplot as plt\n","import cv2\n","from PIL import Image\n","import editdistance\n","from tqdm import tqdm"]},{"cell_type":"markdown","metadata":{},"source":["## Const"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2022-07-25T09:04:00.785728Z","iopub.status.busy":"2022-07-25T09:04:00.784975Z","iopub.status.idle":"2022-07-25T09:04:00.795324Z","shell.execute_reply":"2022-07-25T09:04:00.79453Z","shell.execute_reply.started":"2022-07-25T09:04:00.785687Z"},"trusted":true},"outputs":[],"source":["DIR = './' # work directory\n","PATH_TEST_DIR = '/home/admin/OCR/text-recognition/dataset/test/'\n","PATH_TEST_LABELS =  '/home/admin/OCR/text-recognition/dataset/test.tsv'\n","PATH_TRAIN_DIR =  '/home/admin/OCR/text-recognition/dataset/train/'\n","PATH_TRAIN_LABELS =  '/home/admin/OCR/text-recognition/dataset/train.tsv'\n","PREDICT_PATH = \"/home/admin/OCR/text-recognition/dataset/test/\"\n","CHECKPOINT_PATH = DIR\n","WEIGHTS_PATH =  \"/home/admin/OCR/text-recognition/pretrained_models/ocr_transformer_4h2l_simple_conv_64x256.pt\"\n","PATH_TEST_RESULTS = DIR+'/test_result.tsv'\n","TRAIN_LOG = DIR+'train_log.tsv'"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2022-07-25T09:04:00.803624Z","iopub.status.busy":"2022-07-25T09:04:00.800959Z","iopub.status.idle":"2022-07-25T09:04:00.820991Z","shell.execute_reply":"2022-07-25T09:04:00.819995Z","shell.execute_reply.started":"2022-07-25T09:04:00.803584Z"},"trusted":true},"outputs":[],"source":["\n","MODEL = 'model2'\n","HIDDEN = 512\n","ENC_LAYERS = 2\n","DEC_LAYERS = 2\n","N_HEADS = 4\n","LENGTH = 42\n","ALPHABET = ['PAD', 'SOS', ' ', '!', '\"', '%', '(', ')', ',', '-', '.', '/',\n","            '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?',\n","            '[', ']', '«', '»', 'А', 'Б', 'В', 'Г', 'Д', 'Е', 'Ж', 'З', 'И',\n","            'Й', 'К', 'Л', 'М', 'Н', 'О', 'П', 'Р', 'С', 'Т', 'У', 'Ф', 'Х',\n","            'Ц', 'Ч', 'Ш', 'Щ', 'Э', 'Ю', 'Я', 'а', 'б', 'в', 'г', 'д', 'е',\n","            'ж', 'з', 'и', 'й', 'к', 'л', 'м', 'н', 'о', 'п', 'р', 'с', 'т',\n","            'у', 'ф', 'х', 'ц', 'ч', 'ш', 'щ', 'ъ', 'ы', 'ь', 'э', 'ю', 'я',\n","            'ё', 'EOS']\n","\n","\n","BATCH_SIZE = 8\n","DROPOUT = 0.2\n","N_EPOCHS = 1\n","CHECKPOINT_FREQ = 1\n","DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","RANDOM_SEED = 42\n","SCHUDULER_ON = True\n","PATIENCE = 3\n","OPTIMIZER_NAME = \"SGD\"\n","LR = 2e-6\n","\n","\n","CASE = False\n","PUNCT = False\n","\n","\n","WIDTH = 256\n","HEIGHT = 64\n","CHANNELS = 1"]},{"cell_type":"markdown","metadata":{},"source":["## Utils"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["class PositionalEncoding(torch.nn.Module):\n","    def __init__(self, d_model, dropout=0.1, max_len=5000):\n","        super(PositionalEncoding, self).__init__()\n","        self.dropout = torch.nn.Dropout(p=dropout)\n","        self.scale = torch.nn.Parameter(torch.ones(1))\n","\n","        pe = torch.zeros(max_len, d_model)\n","        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(\n","            0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        pe[:, 1::2] = torch.cos(position * div_term)\n","        pe = pe.unsqueeze(0).transpose(0, 1)\n","        self.register_buffer('pe', pe)\n","\n","    def forward(self, x):\n","        x = x + self.scale * self.pe[:x.size(0), :]\n","        return self.dropout(x)\n","\n","\n","def process_data(image_dir, labels_dir, ignore=[]):\n","    \"\"\"\n","    params\n","    ---\n","    image_dir : str\n","      path to directory with images\n","    labels_dir : str\n","      path to tsv file with labels\n","    returns\n","    ---\n","    img2label : dict\n","      keys are names of images and values are correspondent labels\n","    chars : list\n","      all unique chars used in data\n","    all_labels : list\n","    \"\"\"\n","\n","    chars = []\n","    img2label = dict()\n","\n","    raw = open(labels_dir, 'r', encoding='utf-8').read()\n","    temp = raw.split('\\n')\n","    for t in temp:\n","        try:\n","            x = t.split('\\t')\n","            flag = False\n","            for item in ignore:\n","                if item in x[1]:\n","                    flag = True\n","            if flag == False:\n","                img2label[image_dir + x[0]] = x[1]\n","                for char in x[1]:\n","                    if char not in chars:\n","                        chars.append(char)\n","        except:\n","            print('ValueError:', x)\n","            pass\n","\n","    all_labels = sorted(list(set(list(img2label.values()))))\n","    chars.sort()\n","    chars = ['PAD', 'SOS'] + chars + ['EOS']\n","\n","    return img2label, chars, all_labels\n","\n","\n","def indicies_to_text(indexes, idx2char):\n","    text = \"\".join([idx2char[i] for i in indexes])\n","    text = text.replace('EOS', '').replace('PAD', '').replace('SOS', '')\n","    return text\n","\n","\n","def char_error_rate(p_seq1, p_seq2):\n","    \"\"\"\n","    params\n","    ---\n","    p_seq1 : str\n","    p_seq2 : str\n","    returns\n","    ---\n","    cer : float\n","    \"\"\"\n","    p_vocab = set(p_seq1 + p_seq2)\n","    p2c = dict(zip(p_vocab, range(len(p_vocab))))\n","    c_seq1 = [chr(p2c[p]) for p in p_seq1]\n","    c_seq2 = [chr(p2c[p]) for p in p_seq2]\n","    return editdistance.eval(''.join(c_seq1),\n","                             ''.join(c_seq2)) / max(len(c_seq1), len(c_seq2))\n","\n","\n","def process_image(img):\n","    \"\"\"\n","    params:\n","    ---\n","    img : np.array\n","    returns\n","    ---\n","    img : np.array\n","    \"\"\"\n","    w, h, _ = img.shape\n","    new_w = HEIGHT\n","    new_h = int(h * (new_w / w))\n","    img = cv2.resize(img, (new_h, new_w))\n","    w, h, _ = img.shape\n","\n","    img = img.astype('float32')\n","\n","    new_h = WIDTH\n","    if h < new_h:\n","        add_zeros = np.full((w, new_h - h, 3), 255)\n","        img = np.concatenate((img, add_zeros), axis=1)\n","\n","    if h > new_h:\n","        img = cv2.resize(img, (new_h, new_w))\n","\n","    return img\n","\n","\n","def generate_data(img_paths):\n","    \"\"\"\n","    params\n","    ---\n","    names : list of str\n","        paths to images\n","    returns\n","    ---\n","    data_images : list of np.array\n","        images in np.array format\n","    \"\"\"\n","    data_images = []\n","    for path in tqdm(img_paths):\n","        img = np.asarray(Image.open(path).convert('RGB'))\n","        try:\n","            img = process_image(img)\n","            data_images.append(img.astype('uint8'))\n","        except:\n","            print(path)\n","            img = process_image(img)\n","    return data_images\n","\n","\n","def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","\n","def evaluate(model, criterion, loader, case=True, punct=True):\n","    \"\"\"\n","    params\n","    ---\n","    model : nn.Module\n","    criterion : nn.Object\n","    loader : torch.utils.data.DataLoader\n","\n","    returns\n","    ---\n","    epoch_loss / len(loader) : float\n","        overall loss\n","    \"\"\"\n","    model.eval()\n","    metrics = {'loss': 0, 'wer': 0, 'cer': 0}\n","    result = {'true': [], 'predicted': [], 'wer': []}\n","    with torch.no_grad():\n","        for (src, trg) in loader:\n","            src, trg = src.to(DEVICE), trg.to(DEVICE)\n","            logits = model(src, trg[:-1, :])\n","            loss = criterion(logits.view(-1, logits.shape[-1]), torch.reshape(trg[1:, :], (-1,)))\n","            out_indexes = model.predict(src)\n","\n","            true_phrases = [indicies_to_text(trg.T[i][1:], ALPHABET) for i in range(BATCH_SIZE)]\n","            pred_phrases = [indicies_to_text(out_indexes[i], ALPHABET) for i in range(BATCH_SIZE)]\n","\n","            if not case:\n","                true_phrases = [phrase.lower() for phrase in true_phrases]\n","                pred_phrases = [phrase.lower() for phrase in pred_phrases]\n","            if not punct:\n","                true_phrases = [phrase.translate(str.maketrans('', '', string.punctuation))\\\n","                                for phrase in true_phrases]\n","                pred_phrases = [phrase.translate(str.maketrans('', '', string.punctuation))\\\n","                                for phrase in pred_phrases]\n","\n","            metrics['loss'] += loss.item()\n","            metrics['cer'] += sum([char_error_rate(true_phrases[i], pred_phrases[i]) \\\n","                        for i in range(BATCH_SIZE)])/BATCH_SIZE\n","            metrics['wer'] += sum([int(true_phrases[i] != pred_phrases[i]) \\\n","                        for i in range(BATCH_SIZE)])/BATCH_SIZE\n","\n","            for i in range(len(true_phrases)):\n","              result['true'].append(true_phrases[i])\n","              result['predicted'].append(pred_phrases[i])\n","              result['wer'].append(char_error_rate(true_phrases[i], pred_phrases[i]))\n","\n","    for key in metrics.keys():\n","      metrics[key] /= len(loader)\n","\n","    return metrics, result\n","\n","\n","def prediction(model, test_dir, char2idx, idx2char):\n","    \"\"\"\n","    params\n","    ---\n","    model : nn.Module\n","    test_dir : str\n","        path to directory with images\n","    char2idx : dict\n","        map from chars to indicies\n","    id2char : dict\n","        map from indicies to chars\n","\n","    returns\n","    ---\n","    preds : dict\n","        key : name of image in directory\n","        value : dict with keys ['p_value', 'predicted_label']\n","    \"\"\"\n","    preds = {}\n","    os.makedirs('/home/admin/OCR/text-recognition' + '/output', exist_ok=True)\n","    model.eval()\n","\n","    with torch.no_grad():\n","        for filename in os.listdir(test_dir):\n","            img = Image.open(test_dir + filename).convert('RGB')\n","\n","            img = process_image(np.asarray(img)).astype('uint8')\n","            img = img / img.max()\n","            img = np.transpose(img, (2, 0, 1))\n","\n","            src = torch.FloatTensor(img).unsqueeze(0).to(DEVICE)\n","            if CHANNELS == 1:\n","              src = transforms.Grayscale(CHANNELS)(src)\n","            out_indexes = model.predict(src)\n","            pred = indicies_to_text(out_indexes[0], idx2char)\n","            preds[filename] = pred\n","\n","    return preds\n","\n","\n","class ToTensor(object):\n","    def __init__(self, X_type=None, Y_type=None):\n","        self.X_type = X_type\n","\n","    def __call__(self, X):\n","        X = X.transpose((2, 0, 1))\n","        X = torch.from_numpy(X)\n","        if self.X_type is not None:\n","            X = X.type(self.X_type)\n","        return X\n","\n","\n","def log_config(model):\n","    print('transformer layers: {}'.format(model.enc_layers))\n","    print('transformer heads: {}'.format(model.transformer.nhead))\n","    print('hidden dim: {}'.format(model.decoder.embedding_dim))\n","    print('num classes: {}'.format(model.decoder.num_embeddings))\n","    print('backbone: {}'.format(model.backbone_name))\n","    print('dropout: {}'.format(model.pos_encoder.dropout.p))\n","    print(f'{count_parameters(model):,} trainable parameters')\n","\n","\n","def log_metrics(metrics, path_to_logs=None):\n","    if path_to_logs != None:\n","      f = open(path_to_logs, 'a')\n","    if metrics['epoch'] == 1:\n","      if path_to_logs != None:\n","        f.write('Epoch\\tTrain_loss\\tValid_loss\\tCER\\tWER\\tTime\\n')\n","      print('Epoch   Train_loss   Valid_loss   CER   WER    Time    LR')\n","      print('-----   -----------  ----------   ---   ---    ----    ---')\n","    print('{:02d}       {:.2f}         {:.2f}       {:.2f}   {:.2f}   {:.2f}   {:.7f}'.format(\\\n","        metrics['epoch'], metrics['train_loss'], metrics['loss'], metrics['cer'], \\\n","        metrics['wer'], metrics['time'], metrics['lr']))\n","    if path_to_logs != None:\n","      f.write(str(metrics['epoch'])+'\\t'+str(metrics['train_loss'])+'\\t'+str(metrics['loss'])+'\\t'+str(metrics['cer'])+'\\t'+str(metrics['wer'])+'\\t'+str(metrics['time'])+'\\n')\n","      f.close()\n","\n","\n","# plot images\n","def show_img_grid(images, labels, N):\n","    n = int(N**(0.5))\n","    k = 0\n","    f, axarr = plt.subplots(n,n,figsize=(10,10))\n","    for i in range(n):\n","        for j in range(n):\n","            axarr[i,j].set_title(labels[k])\n","            axarr[i,j].imshow(images[k])\n","            k += 1"]},{"cell_type":"markdown","metadata":{},"source":["## Dataset"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2022-07-25T09:04:00.918018Z","iopub.status.busy":"2022-07-25T09:04:00.915803Z","iopub.status.idle":"2022-07-25T09:04:00.955123Z","shell.execute_reply":"2022-07-25T09:04:00.954318Z","shell.execute_reply.started":"2022-07-25T09:04:00.917979Z"},"trusted":true},"outputs":[],"source":["# text to array of indicies\n","def text_to_labels(s, char2idx):\n","    return [char2idx['SOS']] + [char2idx[i] for i in s if i in char2idx.keys()] + [char2idx['EOS']]\n","\n","# store list of images' names (in directory) and does some operations with images\n","class TextLoader(torch.utils.data.Dataset):\n","    def __init__(self, images_name, labels, transforms, char2idx, idx2char):\n","        \"\"\"\n","        params\n","        ---\n","        images_name : list\n","            list of names of images (paths to images)\n","        labels : list\n","            list of labels to correspondent images from images_name list\n","        char2idx : dict\n","        idx2char : dict\n","        \"\"\"\n","        self.images_name = images_name\n","        self.labels = labels\n","        self.char2idx = char2idx\n","        self.idx2char = idx2char\n","        self.transform = transforms\n","\n","    def _transform(self, X):\n","        j = np.random.randint(0, 3, 1)[0]\n","        if j == 0:\n","            return self.transform(X)\n","        if j == 1:\n","            return tt(ld(vignet(X)))\n","        if j == 2:\n","            return tt(ld(un(X)))\n","\n","\n","    # shows some stats about dataset\n","    def get_info(self):\n","        N = len(self.labels)\n","        max_len = -1\n","        for label in self.labels:\n","            if len(label) > max_len:\n","                max_len = len(label)\n","        counter = Counter(''.join(self.labels))\n","        counter = dict(sorted(counter.items(), key=lambda item: item[1]))\n","        print(\n","            'Size of dataset: {}\\nMax length of expression: {}\\nThe most common char: {}\\nThe least common char: {}'.format( \\\n","                N, max_len, list(counter.items())[-1], list(counter.items())[0]))\n","\n","    def __getitem__(self, index):\n","        img = self.images_name[index]\n","        img = self.transform(img)\n","        img = img / img.max()\n","        img = img ** (random.random() * 0.7 + 0.6)\n","\n","        label = text_to_labels(self.labels[index], self.char2idx)\n","        return (torch.FloatTensor(img), torch.LongTensor(label))\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","\n","# MAKE TEXT TO BE THE SAME LENGTH\n","class TextCollate():\n","    def __call__(self, batch):\n","        x_padded = []\n","        y_padded = torch.LongTensor(LENGTH, len(batch))\n","        y_padded.zero_()\n","\n","        for i in range(len(batch)):\n","            x_padded.append(batch[i][0].unsqueeze(0))\n","            y = batch[i][1]\n","            y_padded[:y.size(0), i] = y\n","\n","        x_padded = torch.cat(x_padded)\n","        return x_padded, y_padded\n","\n","\n","p = Augmentor.Pipeline()\n","p.shear(max_shear_left=2, max_shear_right=2, probability=0.7)\n","p.random_distortion(probability=1.0, grid_width=3, grid_height=3, magnitude=11)\n","\n","TRAIN_TRANSFORMS = transforms.Compose([\n","            transforms.ToPILImage(),\n","            transforms.Grayscale(CHANNELS),\n","            p.torch_transform(),  # random distortion and shear\n","            transforms.ColorJitter(contrast=(0.5,1),saturation=(0.5,1)),\n","            transforms.RandomRotation(degrees=(-9, 9)),\n","            transforms.RandomAffine(10, None, [0.6 ,1] ,3),\n","            #transforms.transforms.GaussianBlur(3, sigma=(0.1, 1.9)),\n","            transforms.ToTensor()\n","        ])\n","\n","TEST_TRANSFORMS = transforms.Compose([\n","            transforms.ToPILImage(),\n","            transforms.Grayscale(CHANNELS),\n","            transforms.ToTensor()\n","        ])"]},{"cell_type":"markdown","metadata":{},"source":["## Model"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2022-07-25T09:04:00.962039Z","iopub.status.busy":"2022-07-25T09:04:00.95935Z","iopub.status.idle":"2022-07-25T09:04:01.011336Z","shell.execute_reply":"2022-07-25T09:04:01.010391Z","shell.execute_reply.started":"2022-07-25T09:04:00.961985Z"},"trusted":true},"outputs":[],"source":["class TransformerModel(nn.Module):\n","    def __init__(self, outtoken, hidden, enc_layers=1, dec_layers=1, nhead=1, dropout=0.1):\n","        super(TransformerModel, self).__init__()\n","\n","        self.enc_layers = enc_layers\n","        self.dec_layers = dec_layers\n","        self.backbone_name = 'conv(64)->conv(64)->conv(128)->conv(256)->conv(256)->conv(512)->conv(512)'\n","\n","        self.conv0 = Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        self.conv1 = Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        self.conv2 = Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 1), padding=(1, 1))\n","        self.conv3 = Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        self.conv4 = Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 1), padding=(1, 1))\n","        self.conv5 = Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        self.conv6 = Conv2d(512, 512, kernel_size=(2, 1), stride=(1, 1))\n","\n","        self.pool1 = MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","        self.pool3 = MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","        self.pool5 = MaxPool2d(kernel_size=(2, 2), stride=(2, 1), padding=(0, 1), dilation=1, ceil_mode=False)\n","\n","        self.bn0 = BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        self.bn1 = BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        self.bn2 = BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        self.bn3 = BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        self.bn4 = BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        self.bn5 = BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        self.bn6 = BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","\n","        self.activ = LeakyReLU()\n","\n","        self.pos_encoder = PositionalEncoding(hidden, dropout)\n","        self.decoder = nn.Embedding(outtoken, hidden)\n","        self.pos_decoder = PositionalEncoding(hidden, dropout)\n","        self.transformer = nn.Transformer(d_model=hidden, nhead=nhead, num_encoder_layers=enc_layers,\n","                                          num_decoder_layers=dec_layers, dim_feedforward=hidden * 4, dropout=dropout)\n","\n","        self.fc_out = nn.Linear(hidden, outtoken)\n","        self.src_mask = None\n","        self.trg_mask = None\n","        self.memory_mask = None\n","\n","        log_config(self)\n","\n","    def generate_square_subsequent_mask(self, sz):\n","        mask = torch.triu(torch.ones(sz, sz, device=DEVICE), 1)\n","        mask = mask.masked_fill(mask == 1, float('-inf'))\n","        return mask\n","\n","    def make_len_mask(self, inp):\n","        return (inp == 0).transpose(0, 1)\n","\n","    def _get_features(self, src):\n","        '''\n","        params\n","        ---\n","        src : Tensor [64, 3, 64, 256] : [B,C,H,W]\n","            B - batch, C - channel, H - height, W - width\n","        returns\n","        ---\n","        x : Tensor : [W,B,CH]\n","        '''\n","        x = self.activ(self.bn0(self.conv0(src)))\n","        x = self.pool1(self.activ(self.bn1(self.conv1(x))))\n","        x = self.activ(self.bn2(self.conv2(x)))\n","        x = self.pool3(self.activ(self.bn3(self.conv3(x))))\n","        x = self.activ(self.bn4(self.conv4(x)))\n","        x = self.pool5(self.activ(self.bn5(self.conv5(x))))\n","        x = self.activ(self.bn6(self.conv6(x)))\n","        x = x.permute(0, 3, 1, 2).flatten(2).permute(1, 0, 2)\n","        return x\n","\n","    def predict(self, batch):\n","        '''\n","        params\n","        ---\n","        batch : Tensor [64, 3, 64, 256] : [B,C,H,W]\n","            B - batch, C - channel, H - height, W - width\n","\n","        returns\n","        ---\n","        result : List [64, -1] : [B, -1]\n","            preticted sequences of tokens' indexes\n","        '''\n","        result = []\n","        for item in batch:\n","          x = self._get_features(item.unsqueeze(0))\n","          memory = self.transformer.encoder(self.pos_encoder(x))\n","          out_indexes = [ALPHABET.index('SOS'), ]\n","          for i in range(100):\n","              trg_tensor = torch.LongTensor(out_indexes).unsqueeze(1).to(DEVICE)\n","              output = self.fc_out(self.transformer.decoder(self.pos_decoder(self.decoder(trg_tensor)), memory))\n","\n","              out_token = output.argmax(2)[-1].item()\n","              out_indexes.append(out_token)\n","              if out_token == ALPHABET.index('EOS'):\n","                  break\n","          result.append(out_indexes)\n","        return result\n","\n","    def forward(self, src, trg):\n","        '''\n","        params\n","        ---\n","        src : Tensor [64, 3, 64, 256] : [B,C,H,W]\n","            B - batch, C - channel, H - height, W - width\n","        trg : Tensor [13, 64] : [L,B]\n","            L - max length of label\n","        '''\n","        if self.trg_mask is None or self.trg_mask.size(0) != len(trg):\n","            self.trg_mask = self.generate_square_subsequent_mask(len(trg)).to(trg.device)\n","\n","        x = self._get_features(src)\n","        src_pad_mask = self.make_len_mask(x[:, :, 0])\n","        src = self.pos_encoder(x)\n","        trg_pad_mask = self.make_len_mask(trg)\n","        trg = self.decoder(trg)\n","        trg = self.pos_decoder(trg)\n","\n","        output = self.transformer(src, trg, src_mask=self.src_mask, tgt_mask=self.trg_mask,\n","                                  memory_mask=self.memory_mask,\n","                                  src_key_padding_mask=src_pad_mask, tgt_key_padding_mask=trg_pad_mask,\n","                                  memory_key_padding_mask=src_pad_mask)\n","        output = self.fc_out(output)\n","\n","        return output"]},{"cell_type":"markdown","metadata":{},"source":["## Train"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2022-07-25T09:04:01.018704Z","iopub.status.busy":"2022-07-25T09:04:01.01613Z","iopub.status.idle":"2022-07-25T09:04:01.035547Z","shell.execute_reply":"2022-07-25T09:04:01.033997Z","shell.execute_reply.started":"2022-07-25T09:04:01.018668Z"},"trusted":true},"outputs":[],"source":["def train(model, optimizer, criterion, train_loader):\n","\n","    \"\"\"\n","    params\n","    ---\n","    model : nn.Module\n","    optimizer : nn.Object\n","    criterion : nn.Object\n","    train_loader : torch.utils.data.DataLoader\n","    returns\n","    ---\n","    epoch_loss / len(train_loader) : float\n","        overall loss\n","    \"\"\"\n","    model.train()\n","    epoch_loss = 0\n","    for src, trg in train_loader:\n","        src, trg = src.to(DEVICE), trg.to(DEVICE)\n","        optimizer.zero_grad()\n","        output = model(src, trg[:-1, :])\n","\n","        loss = criterion(output.view(-1, output.shape[-1]), torch.reshape(trg[1:, :], (-1,)))\n","        loss.backward()\n","        optimizer.step()\n","        epoch_loss += loss.item()\n","\n","    return epoch_loss / len(train_loader)\n","\n","\n","# GENERAL FUNCTION FROM TRAINING AND VALIDATION\n","def fit(model, optimizer, scheduler, criterion, train_loader, val_loader, start_epoch=0, end_epoch=24):\n","    metrics = []\n","    for epoch in range(start_epoch, end_epoch):\n","        epoch_metrics = {}\n","        start_time = time()\n","        train_loss = train(model, optimizer, criterion, train_loader)\n","        end_time = time()\n","        epoch_metrics, _ = evaluate(model, criterion, val_loader)\n","        epoch_metrics['train_loss'] = train_loss\n","        epoch_metrics['epoch'] = epoch\n","        epoch_metrics['time'] = end_time - start_time\n","        epoch_metrics['lr'] = optimizer.param_groups[0][\"lr\"]\n","        metrics.append(epoch_metrics)\n","        log_metrics(epoch_metrics, TRAIN_LOG)\n","        if scheduler != None:\n","            scheduler.step(epoch_metrics['loss'])\n","    return metrics"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2022-07-25T09:04:01.042375Z","iopub.status.busy":"2022-07-25T09:04:01.039736Z","iopub.status.idle":"2022-07-25T09:09:15.424683Z","shell.execute_reply":"2022-07-25T09:09:15.423653Z","shell.execute_reply.started":"2022-07-25T09:04:01.042278Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["loading dataset /home/admin/OCR/simple/dataset/train/ ...\n"]},{"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/home/admin/OCR/simple/dataset/train.tsv'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[1;32m/home/admin/OCR/text-recognition/train.ipynb Cell 13\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B84.201.138.253/home/admin/OCR/text-recognition/train.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m idx2char \u001b[39m=\u001b[39m {idx: char \u001b[39mfor\u001b[39;00m idx, char \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(ALPHABET)}\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B84.201.138.253/home/admin/OCR/text-recognition/train.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mloading dataset \u001b[39m\u001b[39m{\u001b[39;00mPATH_TRAIN_DIR\u001b[39m}\u001b[39;00m\u001b[39m ...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B84.201.138.253/home/admin/OCR/text-recognition/train.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m img2label, _, all_words \u001b[39m=\u001b[39m process_data(PATH_TRAIN_DIR, PATH_TRAIN_LABELS)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B84.201.138.253/home/admin/OCR/text-recognition/train.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m img_names, labels \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(img2label\u001b[39m.\u001b[39mkeys()), \u001b[39mlist\u001b[39m(img2label\u001b[39m.\u001b[39mvalues())\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B84.201.138.253/home/admin/OCR/text-recognition/train.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m X_train \u001b[39m=\u001b[39m generate_data(img_names)\n","\u001b[1;32m/home/admin/OCR/text-recognition/train.ipynb Cell 13\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B84.201.138.253/home/admin/OCR/text-recognition/train.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=37'>38</a>\u001b[0m chars \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B84.201.138.253/home/admin/OCR/text-recognition/train.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=38'>39</a>\u001b[0m img2label \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m()\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B84.201.138.253/home/admin/OCR/text-recognition/train.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=40'>41</a>\u001b[0m raw \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(labels_dir, \u001b[39m'\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m, encoding\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39mread()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B84.201.138.253/home/admin/OCR/text-recognition/train.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=41'>42</a>\u001b[0m temp \u001b[39m=\u001b[39m raw\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B84.201.138.253/home/admin/OCR/text-recognition/train.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m temp:\n","File \u001b[0;32m~/hackaton_env/lib/python3.9/site-packages/IPython/core/interactiveshell.py:308\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[1;32m    302\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    303\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    304\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    305\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m     )\n\u001b[0;32m--> 308\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/admin/OCR/simple/dataset/train.tsv'"]}],"source":["random.seed(RANDOM_SEED)\n","torch.manual_seed(RANDOM_SEED)\n","torch.cuda.manual_seed(RANDOM_SEED)\n","\n","char2idx = {char: idx for idx, char in enumerate(ALPHABET)}\n","idx2char = {idx: char for idx, char in enumerate(ALPHABET)}\n","\n","print(f\"loading dataset {PATH_TRAIN_DIR} ...\")\n","img2label, _, all_words = process_data(PATH_TRAIN_DIR, PATH_TRAIN_LABELS)\n","img_names, labels = list(img2label.keys()), list(img2label.values())\n","X_train = generate_data(img_names)\n","y_train = labels\n","\n","train_dataset = TextLoader(X_train, y_train, TRAIN_TRANSFORMS, char2idx, idx2char)\n","train_loader = torch.utils.data.DataLoader(train_dataset, shuffle=True,\n","                                           batch_size=BATCH_SIZE, pin_memory=True,\n","                                           drop_last=True, collate_fn=TextCollate())\n","\n","print(f\"loading dataset {PATH_TEST_DIR} ...\")\n","img2label, _, all_words = process_data(PATH_TEST_DIR, PATH_TEST_LABELS)\n","img_names, labels = list(img2label.keys()), list(img2label.values())\n","X_test = generate_data(img_names)\n","y_test = labels\n","\n","test_dataset = TextLoader(X_test, y_test, TEST_TRANSFORMS, char2idx ,idx2char)\n","test_loader = torch.utils.data.DataLoader(test_dataset, shuffle=True,\n","                                           batch_size=BATCH_SIZE, pin_memory=True,\n","                                           drop_last=True, collate_fn=TextCollate())\n","\n","print(\"TRAIN DATASET:\")\n","train_dataset.get_info()\n","print(\"\\nTEST DATASET:\")\n","test_dataset.get_info()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["transformer layers: 2\n","transformer heads: 4\n","hidden dim: 512\n","num classes: 92\n","backbone: conv(64)->conv(64)->conv(128)->conv(256)->conv(256)->conv(512)->conv(512)\n","dropout: 0.2\n","19,838,174 trainable parameters\n","checkpoints are saved in ./ every 1 epochs\n"]}],"source":["model = TransformerModel(len(ALPHABET), hidden=HIDDEN, enc_layers=ENC_LAYERS, dec_layers=DEC_LAYERS,\n","                          nhead=N_HEADS, dropout=DROPOUT).to(DEVICE)\n","\n","criterion = torch.nn.CrossEntropyLoss(ignore_index=char2idx['PAD'])\n","optimizer = torch.optim.__getattribute__(OPTIMIZER_NAME)(model.parameters(), lr=LR)\n","\n","if SCHUDULER_ON:\n","    scheduler =torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=PATIENCE)\n","else:\n","    scheduler = None\n","\n","print(f'checkpoints are saved in {CHECKPOINT_PATH} every {CHECKPOINT_FREQ} epochs')\n","for epoch in range(1, N_EPOCHS, CHECKPOINT_FREQ):\n","    fit(model, optimizer, scheduler, criterion, train_loader, test_loader, epoch, epoch+CHECKPOINT_FREQ)\n","    torch.save(model.state_dict(), CHECKPOINT_PATH+'checkpoint_{}.pt'.format(epoch+CHECKPOINT_FREQ))"]},{"cell_type":"markdown","metadata":{},"source":["## Test"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-07-25T09:40:23.663155Z","iopub.status.busy":"2022-07-25T09:40:23.66235Z","iopub.status.idle":"2022-07-25T09:41:01.85733Z","shell.execute_reply":"2022-07-25T09:41:01.856341Z","shell.execute_reply.started":"2022-07-25T09:40:23.663118Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["transformer layers: 2\n","transformer heads: 4\n","hidden dim: 512\n","num classes: 92\n","backbone: conv(64)->conv(64)->conv(128)->conv(256)->conv(256)->conv(512)->conv(512)\n","dropout: 0.2\n","19,838,174 trainable parameters\n"]},{"ename":"ZeroDivisionError","evalue":"division by zero","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)","\u001b[1;32m/home/admin/OCR/simple/train.ipynb Cell 17\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B84.201.138.253/home/admin/OCR/simple/train.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# evaluate\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B84.201.138.253/home/admin/OCR/simple/train.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m criterion \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mCrossEntropyLoss(ignore_index\u001b[39m=\u001b[39mchar2idx[\u001b[39m'\u001b[39m\u001b[39mPAD\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B84.201.138.253/home/admin/OCR/simple/train.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m metrics, result \u001b[39m=\u001b[39m evaluate(model, criterion, test_loader, case\u001b[39m=\u001b[39;49mCASE, punct\u001b[39m=\u001b[39;49mPUNCT)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B84.201.138.253/home/admin/OCR/simple/train.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mif\u001b[39;00m PATH_TEST_RESULTS \u001b[39m!=\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B84.201.138.253/home/admin/OCR/simple/train.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m   f \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(PATH_TEST_RESULTS, \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m)\n","\u001b[1;32m/home/admin/OCR/simple/train.ipynb Cell 17\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B84.201.138.253/home/admin/OCR/simple/train.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=193'>194</a>\u001b[0m           result[\u001b[39m'\u001b[39m\u001b[39mwer\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mappend(char_error_rate(true_phrases[i], pred_phrases[i]))\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B84.201.138.253/home/admin/OCR/simple/train.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=195'>196</a>\u001b[0m \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m metrics\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2B84.201.138.253/home/admin/OCR/simple/train.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=196'>197</a>\u001b[0m   metrics[key] \u001b[39m/\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(loader)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B84.201.138.253/home/admin/OCR/simple/train.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=198'>199</a>\u001b[0m \u001b[39mreturn\u001b[39;00m metrics, result\n","\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"]}],"source":["# load weights\n","model = TransformerModel(len(ALPHABET), hidden=HIDDEN, enc_layers=ENC_LAYERS, dec_layers=DEC_LAYERS,\n","                          nhead=N_HEADS, dropout=DROPOUT).to(DEVICE)\n","model.load_state_dict(torch.load(WEIGHTS_PATH))\n","\n","# evaluate\n","criterion = torch.nn.CrossEntropyLoss(ignore_index=char2idx['PAD'])\n","metrics, result = evaluate(model, criterion, test_loader, case=CASE, punct=PUNCT)\n","\n","if PATH_TEST_RESULTS != None:\n","  f = open(PATH_TEST_RESULTS, 'w')\n","  f.write(\"true\\tpredicted\\twer\\n\")\n","  for i in range(len(result['true'])):\n","    f.write(result['true'][i]+\\\n","            '\\t'+result['predicted'][i]+\\\n","            '\\t'+str(result['wer'][i])+'\\n')\n","print(f'PUNCT: {PUNCT}, CASE: {CASE}')\n","print(metrics)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-07-25T09:41:01.860046Z","iopub.status.busy":"2022-07-25T09:41:01.859001Z","iopub.status.idle":"2022-07-25T09:41:01.88979Z","shell.execute_reply":"2022-07-25T09:41:01.888615Z","shell.execute_reply.started":"2022-07-25T09:41:01.859991Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>true</th>\n","      <th>predicted</th>\n","      <th>wer</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>тогда</td>\n","      <td>тогда</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>начальники</td>\n","      <td>начальнику</td>\n","      <td>0.100000</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>мамедов а г</td>\n","      <td>мамевов а</td>\n","      <td>0.272727</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>иванову</td>\n","      <td>иванову</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>обменом</td>\n","      <td>обменом</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>классного</td>\n","      <td>классного</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>в шрм</td>\n","      <td>в шом</td>\n","      <td>0.200000</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>особое</td>\n","      <td>особое</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>сумма</td>\n","      <td>сумма</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>кафедра зоологии</td>\n","      <td>кафедны 30 гии</td>\n","      <td>0.437500</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["               true       predicted       wer\n","0             тогда           тогда  0.000000\n","1        начальники      начальнику  0.100000\n","2       мамедов а г       мамевов а  0.272727\n","3           иванову         иванову  0.000000\n","4           обменом         обменом  0.000000\n","5         классного       классного  0.000000\n","6             в шрм           в шом  0.200000\n","7            особое          особое  0.000000\n","8             сумма           сумма  0.000000\n","9  кафедра зоологии  кафедны 30 гии  0.437500"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["# read predictions and show it\n","df = pd.read_csv(PATH_TEST_RESULTS, sep='\\t', quoting=3)\n","df.head(10)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-07-25T09:41:01.891696Z","iopub.status.busy":"2022-07-25T09:41:01.891209Z","iopub.status.idle":"2022-07-25T09:41:41.075881Z","shell.execute_reply":"2022-07-25T09:41:41.074767Z","shell.execute_reply.started":"2022-07-25T09:41:01.891658Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["predictions are saved in ./predictions.tsv\n"]}],"source":["preds = prediction(model, PREDICT_PATH, char2idx, idx2char)\n","\n","f = open('/home/admin/OCR/text-recognition'+'/predictions.tsv', 'w')\n","f.write('filename\\tprediction\\n')\n","for item in preds.items():\n","    f.write(item[0]+'\\t'+item[1]+'\\n')\n","f.close()\n","print(f'predictions are saved in {DIR}predictions.tsv')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-07-25T09:41:41.078405Z","iopub.status.busy":"2022-07-25T09:41:41.077962Z","iopub.status.idle":"2022-07-25T09:41:42.000112Z","shell.execute_reply":"2022-07-25T09:41:41.999338Z","shell.execute_reply.started":"2022-07-25T09:41:41.078363Z"},"trusted":true},"outputs":[{"ename":"NameError","evalue":"name 'pd' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32m/home/admin/OCR/text-recognition/train.ipynb Cell 19\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B84.201.138.253/home/admin/OCR/text-recognition/train.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(DIR\u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39m/predictions.tsv\u001b[39m\u001b[39m'\u001b[39m, sep\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m'\u001b[39m, quoting\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B84.201.138.253/home/admin/OCR/text-recognition/train.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m N \u001b[39m=\u001b[39m \u001b[39m9\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B84.201.138.253/home/admin/OCR/text-recognition/train.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m images \u001b[39m=\u001b[39m []\n","\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"]}],"source":["df = pd.read_csv(DIR+'/predictions.tsv', sep='\\t', quoting=3)\n","N = 9\n","images = []\n","labels = []\n","for i in range(N):\n","    idx = random.randint(0, len(df))\n","    image_path = PATH_TEST_DIR + df.iloc[idx]['filename']\n","    predicted_label = df.iloc[idx]['prediction']\n","\n","    images.append(Image.open(image_path))\n","    labels.append(predicted_label)\n","\n","show_img_grid(images, labels, N)"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":2254836,"isSourceIdPinned":true,"sourceId":3976575,"sourceType":"datasetVersion"},{"datasetId":1502872,"isSourceIdPinned":true,"sourceId":3977616,"sourceType":"datasetVersion"}],"dockerImageVersionId":30197,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.18"}},"nbformat":4,"nbformat_minor":4}
